# Notes SAL S13

Session Notes : [S13](https://coding-platform.s3.amazonaws.com/dev/lms/tickets/3494c9a4-870c-4458-8f8e-170857293f27/sVgTRxhyDU0HKWKt.png)

## **1.0 : Introduction to High-Level Design (HLD)**

Building modern systems isn't just about writing code that works. It’s about designing systems that scale, recover from failure, handle traffic smoothly, and stay easy to evolve. That’s where **High-Level Design (HLD)** comes in.

HLD focuses on the **overall structure** of a system — the main components, how they interact, and how data flows between them. It's a blueprint for engineers and architects to think beyond single functions or APIs and consider systems as a whole.

In simple terms, HLD answers:

- What are the core components (frontend, backend, cache, DB, etc.)?
- How are these components connected?
- What are the tradeoffs in different design choices?

When you're asked to "design Instagram feed" or "build a URL shortener," you're being tested on HLD thinking.

As systems grow, architectures become more **complex**. Instead of one server handling everything, real-world systems use multiple services spread across machines, networks, and even continents. Managing that complexity requires a solid grasp of distributed concepts.

---

## **2.0 : Distributed Systems and Performance Metrics**

A **Distributed System** is a collection of machines that work together but appear as a single system to the user. These machines communicate over a network, cooperate to handle requests, and must deal with unpredictable failures like network delays, server crashes, or hardware issues.

Two key performance metrics are essential in distributed systems: **latency** and **throughput**.

**Latency** refers to the **time taken to respond** to a single request. It affects how "fast" the system feels to users. If you open LinkedIn and the feed takes 5 seconds to load, that’s a latency issue. Companies like LinkedIn have optimized their systems to **reduce latency by over 60%**, which directly improves user engagement.

**Throughput** is the **number of requests** a system can handle per second. While latency is about individual request speed, throughput is about how much work the system can do in a given time.

These two often **pull in opposite directions**. Optimizing for one may hurt the other. For example:

- Batch processing improves throughput but increases latency.
- Serving from cache lowers latency but might not scale for thousands of requests per second if poorly managed.

To maintain a balance, systems often use **Rate Limiters** — components that **control how many requests** a user or service can make in a time window. Rate limiting protects systems from abuse and prevents overload during traffic spikes. Techniques include token buckets, leaky buckets, and fixed window counters.

As traffic grows, **scaling** becomes necessary. There are two primary types:

- **Vertical Scaling**: Increase the power (CPU, RAM) of a single machine. It’s simple but limited.
- **Horizontal Scaling**: Add more machines and distribute the load. It's more scalable but also more complex.

Horizontal scaling is what powers large-scale systems like Netflix or Amazon, which handle millions of requests per second across many servers.

---

## **3.0 : Load Balancing and Server Design**

As soon as a system starts using multiple servers, the challenge becomes: **How do we distribute requests across these servers?** This is the job of a **Load Balancer**.

A load balancer is a component that **sits in front of your servers** and forwards incoming requests to them in a balanced manner. Without it, some servers might be overloaded while others sit idle.

One way to distribute requests is through **hashing** — for example, using a user's ID to determine which server they should hit:

```tsx
serverIndex = hash(userId) % numberOfServers;
```

This is called **Normal Hashing**. It’s simple, but it has a critical weakness: if the number of servers changes, **almost all users get remapped to new servers**. This leads to cache misses, dropped sessions, and unnecessary computations.

To solve this, we use **Consistent Hashing**. It places both servers and keys on a circular ring. When a new server is added, only a few keys (users) get remapped to it. This allows smoother scaling and avoids massive data reshuffling.

Beyond balancing load, another key design choice is between **stateful** and **stateless** servers.

- **Stateless Servers** do not store session or user data locally. Each request is treated independently. This makes horizontal scaling easier because any server can handle any request.
- **Stateful Servers** store user data or sessions in memory. This improves speed but makes scaling harder — a user must always be routed to the same server.

Modern systems prefer stateless servers combined with **external storage** (like Redis or databases) for state, allowing flexible scaling and better fault tolerance.